{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set: Advanced Part-of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "all_hansard = pd.read_parquet(\"/scratch/group/oit_research_data/hansard/hansard_20191119.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hansard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hansard['speechdate'] = pd.to_datetime(all_hansard['speechdate'], errors = 'coerce')\n",
    "\n",
    "hansard_1870 = all_hansard[(all_hansard['speechdate'] >= dt.datetime(1870,1,1)) & (all_hansard['speechdate'] <= dt.datetime(1879,12,31))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_hansard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_text = hansard_1870[['sentence_id','text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hansard_text['parsed_text'] = list(nlp.pipe(hansard_text['text'], disable = [\"ent\"], batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "def extractPairs(spacy_doc_object):\n",
    "    pairs = []\n",
    "    for doc in spacy_doc_object['parsed_text']:\n",
    "        for subject in doc:\n",
    "            if subject.dep == nsubj or subject.dep == nsubjpass and subject.head.pos == VERB:\n",
    "                extracted_pairs = subject.text, subject.head.lemma_\n",
    "                concat_extracted_pairs = ' '.join(extracted_pairs)\n",
    "                pairs.append(str(concat_extracted_pairs))\n",
    "    return pairs\n",
    "\n",
    "pairs = extractPairs(hansard_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_pairs = [word.lower() for word in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_pairs[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Gendered Noun-Verb Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = (\"Man,Men,Male,Father,Boy,Uncle,Husband,Actor,Prince,Waiter,He,His,Hisself,Him,Himself,Brother,Count,Dad,Daddy,Duke,Emperor,God,Grandfather,Godfather,Heir,Hero,Host,Husband,King,Master,Nephew,Policeman,Sir,Wizard,Son,Gentleman\")\n",
    "male_nouns = male_words.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_words = (\"Woman,Women,Mother,Girl,Female,Aunt,Wife,Actress,Princess,Waitress,She,Her,Hers,Herself,Bride,Sister,Countess,Mom,Mommy,Empress,Goddess,Grandmother,Godmother,Hostess,Heiress,Queen,Mistress,Madam,Niece,Daughter,Aunt,Witch,Heroine,Lady,Gentlewoman,Maâ€™am\")\n",
    "female_nouns = female_words.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extractPairs(pairs,noun_list):\n",
    "    nounsL = []\n",
    "    for gendered_noun in \n",
    "        regex = re.compile(gendered_noun + \" \")\n",
    "        nouns = [word for word in lowercase_pairs if regex.match(word)]\n",
    "        nounsL.append(nouns)\n",
    "    return nounsL\n",
    "\n",
    "male = extractPairs(lowercase_pairs, male_nouns)\n",
    "female = extractPairs(lowercase_pairs, female_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male[:30]\n",
    "female[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
